from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydub import AudioSegment
import uvicorn
import torch
import torchaudio
import numpy as np
from tensorflow.keras.models import load_model
import os
import logging
import uuid
from transformers import Wav2Vec2Processor, Wav2Vec2Model

# ==========================
# Logging
# ==========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==========================
# FastAPI App
# ==========================
app = FastAPI(
    title="üé§ Wav2Vec2 Emotion Recognition API",
    description="Ses tabanlƒ± duygu tanƒ±ma API'si - Wav2Vec2 modeli ile eƒüitilmi≈ü",
    version="1.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================
# Global Variables
# ==========================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODELS = {}
LABEL_ENCODER = None
WAV2VEC_PROCESSOR = None
WAV2VEC_MODEL = None

# Turkish-English emotion mapping
EN_TO_TR = {
    "neutral": "N√∂tr",
    "calm": "Sakin", 
    "happy": "Mutlu",
    "sad": "√úzg√ºn",
    "angry": "Kƒ±zgƒ±n",
    "fearful": "Endi≈üeli",
    "disgust": "Ho≈ünutsuz",
    "surprised": "≈ûa≈ükƒ±n"
}

# ==========================
# Model Loading
# ==========================
def load_wav2vec_models():
    global WAV2VEC_PROCESSOR, WAV2VEC_MODEL
    try:
        WAV2VEC_PROCESSOR = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base")
        WAV2VEC_MODEL = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base")
        WAV2VEC_MODEL.eval()
        logger.info("‚úÖ Wav2Vec2 models loaded successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to load Wav2Vec2 models: {e}")
        return False

def load_classifier_model():
    global MODELS
    model_path = r"C:\Users\kdrt2\OneDrive\Masa√ºst√º\emotion-recognition-app\apibackend_w2v\wav2vec2_model.h5"
    if not os.path.exists(model_path):
        logger.error(f"‚ùå Model file not found: {model_path}")
        return False
    try:
        model = load_model(model_path, compile=False)
        MODELS["Wav2Vec2"] = model
        logger.info("‚úÖ Wav2Vec2 classifier loaded successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Failed to load classifier: {e}")
        return False


def load_label_encoder():
    global LABEL_ENCODER
    encoder_path = r"C:\Users\kdrt2\OneDrive\Masa√ºst√º\emotion-recognition-app\emotion-recognition-app1\modeller\classes.npy"
    if os.path.exists(encoder_path):
        try:
            LABEL_ENCODER = np.load(encoder_path)
            logger.info(f"‚úÖ Label encoder loaded: {LABEL_ENCODER}")
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not load label encoder: {e}")
    else:
        logger.info("‚ÑπÔ∏è No label encoder found, using default classes")
    return False

@app.on_event("startup")
async def startup_event():
    logger.info("üöÄ Starting Wav2Vec2 Emotion Recognition API")
    load_wav2vec_models()
    load_classifier_model()
    load_label_encoder()
    logger.info("‚úÖ API ready for predictions")

# ==========================
# Helpers
# ==========================
def convert_to_wav(input_path: str) -> str:
    """Convert any input (e.g. webm/opus) to WAV 16kHz mono"""
    try:
        output_path = input_path + "_conv.wav"
        sound = AudioSegment.from_file(input_path)
        sound = sound.set_frame_rate(16000).set_channels(1)
        sound.export(output_path, format="wav")
        return output_path
    except Exception as e:
        logger.error(f"‚ùå Conversion to wav failed: {e}")
        return None

def extract_wav2vec_features(file_path: str) -> np.ndarray:
    try:
        waveform, sample_rate = torchaudio.load(file_path)
        waveform = waveform.mean(dim=0)  # mono
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        inputs = WAV2VEC_PROCESSOR(waveform, sampling_rate=16000, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = WAV2VEC_MODEL(inputs.input_values)
            hidden_states = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
        return hidden_states.squeeze()
    except Exception as e:
        logger.error(f"‚ùå Wav2Vec2 feature extraction failed: {e}")
        return None

def preprocess_for_wav2vec(features: np.ndarray) -> np.ndarray:
    if features is None or len(features.shape) == 0:
        return None
    if len(features.shape) == 1:
        features = features.reshape(1, -1)
    return features

# ==========================
# Endpoints
# ==========================
@app.get("/")
async def root():
    return {
        "message": "üé§ Wav2Vec2 Emotion Recognition API",
        "status": "running",
        "models_loaded": list(MODELS.keys())
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    if not file or not file.filename:
        return {"error": "Dosya y√ºklenmedi"}

    temp_path = os.path.join(BASE_DIR, f"temp_{uuid.uuid4().hex}_{file.filename}")
    try:
        with open(temp_path, "wb") as f:
            f.write(await file.read())
    except Exception as e:
        return {"error": f"Dosya kaydedilemedi: {str(e)}"}

    try:
        # üîπ Zorunlu d√∂n√º≈ü√ºm
        wav_path = convert_to_wav(temp_path)
        if not wav_path:
            return {"error": "Ses d√∂n√º≈üt√ºr√ºlemedi"}

        # √ñzellik √ßƒ±kar
        features = extract_wav2vec_features(wav_path)
        if features is None:
            return {"error": "√ñzellik √ßƒ±karƒ±lamadƒ±"}

        x = preprocess_for_wav2vec(features)
        if x is None:
            return {"error": "√ñzellik i≈ülenemedi"}

        model = MODELS.get("Wav2Vec2")
        if model is None:
            return {"error": "Model y√ºklenmedi"}

        prediction = model.predict(x, verbose=0)
        pred_idx = int(np.argmax(prediction))
        confidence = float(np.max(prediction))

        logger.info(f"üîç Raw prediction vector: {prediction}")
        logger.info(f"‚úÖ Predicted idx: {pred_idx}, confidence: {confidence}")

        if LABEL_ENCODER is not None and pred_idx < len(LABEL_ENCODER):
            predicted_emotion = LABEL_ENCODER[pred_idx]
        else:
            return {"error": "Label encoder not loaded properly"}

        # Get Turkish translation
        prediction_tr = EN_TO_TR.get(predicted_emotion, predicted_emotion)

        return {
            "prediction": predicted_emotion, 
            "prediction_tr": prediction_tr,
            "confidence": confidence
        }



    except Exception as e:
        logger.error(f"Prediction error: {e}")
        return {"error": "Tahmin sƒ±rasƒ±nda hata olu≈ütu"}
    finally:
        for path in [temp_path, temp_path + "_conv.wav"]:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except Exception:
                    pass

# ==========================
# Main
# ==========================
if __name__ == "__main__":
    uvicorn.run("main_w2v:app", host="127.0.0.1", port=8001, reload=True, log_level="info")

